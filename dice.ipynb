{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openneuro-py mne\n",
        "!openneuro-py download --dataset=ds004504"
      ],
      "metadata": {
        "id": "jXeCvo6ezasC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mne\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.signal import butter, filtfilt, welch, csd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def butter_bandpass(lowcut, highcut, fs, order=4):\n",
        "    \"\"\"\n",
        "    Designs a Butterworth bandpass filter.\n",
        "\n",
        "    Parameters:\n",
        "    - lowcut: Lower cutoff frequency of the filter.\n",
        "    - highcut: Upper cutoff frequency of the filter.\n",
        "    - fs: Sampling frequency.\n",
        "    - order: Order of the Butterworth filter.\n",
        "\n",
        "    Returns:\n",
        "    - b, a: Numerator (b) and denominator (a) polynomials of the filter.\n",
        "    \"\"\"\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def filter(data, bands, fs, order=4):\n",
        "    \"\"\"\n",
        "    Applies a Butterworth bandpass filter to the data for each specified frequency band.\n",
        "\n",
        "    Parameters:\n",
        "    - data: Input signal data (1D array).\n",
        "    - bands: List of tuples specifying the frequency bands [(low1, high1), (low2, high2), ...].\n",
        "    - fs: Sampling frequency.\n",
        "    - order: Order of the Butterworth filter.\n",
        "\n",
        "    Returns:\n",
        "    - A 2D array where each row corresponds to the filtered signal for each band.\n",
        "    \"\"\"\n",
        "    ret = []\n",
        "\n",
        "    for low, high in bands:\n",
        "        # Design the Butterworth bandpass filter for the given band\n",
        "        b, a = butter_bandpass(low, high, fs, order=order)\n",
        "        # Apply the filter to the data\n",
        "        filtered_signal = filtfilt(b, a, data)\n",
        "        ret.append(filtered_signal)\n",
        "\n",
        "    # Stack the filtered signals along the first axis\n",
        "    return np.stack(ret, axis=0)\n",
        "\n",
        "diags = pd.read_csv('ds004504/participants.tsv', sep = \"\\t\")[\"Group\"]\n",
        "scc = []\n",
        "rbp = []\n",
        "processed_diags = []\n",
        "\n",
        "print(set(diags))\n",
        "\n",
        "length = 500 * 30\n",
        "bands = [(0.5, 4), (4, 8), (8, 13), (13, 25), (25, 45)]\n",
        "\n",
        "\n",
        "for i, curr_diag in zip(range(1, 89), diags):\n",
        "\n",
        "    # load the file ds004504/derivatives/sub-001/eeg/sub-001_task-eyesclosed_eeg.set\n",
        "    raw = mne.io.read_raw_eeglab(f'ds004504/derivatives/sub-{\"{:03}\".format(i)}/eeg/sub-{\"{:03}\".format(i)}_task-eyesclosed_eeg.set', preload=True)\n",
        "    raw_data = raw.get_data()\n",
        "    # data is of shape (19, num_samples)\n",
        "    # I want to reshape this to (num, 19, length)\n",
        "    num_segments = raw_data.shape[1] // 15000\n",
        "    raw_data = raw_data[:, :num_segments * 15000].reshape(-1, 19, 15000)\n",
        "    raw_data = raw_data.reshape(-1, 19, length)\n",
        "\n",
        "    if curr_diag != \"F\":\n",
        "        for data in raw_data:\n",
        "            filtered = filter(data, bands, 500)\n",
        "            n_epochs = length // 500\n",
        "            filtered = filtered.reshape(5, 19, n_epochs, 500)\n",
        "            filtered = np.moveaxis(filtered, 2, 0)\n",
        "\n",
        "            fs = 500\n",
        "            nperseg = 500\n",
        "            num_channels = 19\n",
        "\n",
        "            freqs, psd = welch(filtered, fs=fs, nperseg=nperseg, axis=-1)\n",
        "            psd_values = np.sum(psd, axis=-1)\n",
        "            relative_band_power = psd_values / np.sum(psd_values, axis=1, keepdims=True)\n",
        "\n",
        "            # Compute SCC values\n",
        "            f_data = np.fft.rfft(filtered, axis=-1)\n",
        "            auto_spectral = np.mean(np.abs(f_data) ** 2, axis=-1)\n",
        "\n",
        "            f_data_expanded1 = f_data[:, :, :, np.newaxis, :]\n",
        "            f_data_expanded2 = f_data[:, :, np.newaxis, :, :]\n",
        "            cross_spectral = f_data_expanded1 * np.conj(f_data_expanded2)\n",
        "            mean_cross_psd = np.mean(np.abs(cross_spectral), axis=-1)\n",
        "\n",
        "            denominator = np.sqrt(auto_spectral[:, :, :, np.newaxis] * auto_spectral[:, :, np.newaxis, :])\n",
        "            coherence_matrix = mean_cross_psd / denominator\n",
        "\n",
        "            diag_indices = np.arange(num_channels)\n",
        "            coherence_matrix[:, :, diag_indices, diag_indices] = 0\n",
        "\n",
        "            coherence_sum = np.sum(coherence_matrix, axis=-1)\n",
        "            scc_values = coherence_sum / (num_channels - 1)\n",
        "\n",
        "            scc.append(scc_values)\n",
        "            rbp.append(relative_band_power)\n",
        "        processed_diags.extend([curr_diag] * raw_data.shape[0])\n",
        "\n",
        "scc = np.array(scc)\n",
        "rbp = np.array(rbp)\n",
        "processed_diags = np.array(processed_diags)\n",
        "\n",
        "print(scc.shape)\n",
        "print(rbp.shape)\n",
        "print(processed_diags.shape)\n"
      ],
      "metadata": {
        "id": "1IolKUgxzoQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch_scc = torch.tensor(scc).to(\"cuda\").float()\n",
        "torch_rbp = torch.tensor(rbp).to(\"cuda\").float()\n",
        "\n",
        "# currently of shape [32, 30, 5, 19]\n",
        "# make to shape [32, 19, 5, 30]\n",
        "torch_scc = torch_scc.permute(0, 3, 1, 2)\n",
        "torch_rbp = torch_rbp.permute(0, 3, 1, 2)"
      ],
      "metadata": {
        "id": "2VveVRiLGWCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_diags = torch.tensor([0 if letter == \"C\" else 1 for letter in processed_diags]).to(\"cuda\").float()"
      ],
      "metadata": {
        "id": "X-cAeLTlGiWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install positional-encodings"
      ],
      "metadata": {
        "id": "ah5-MppfXeIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from positional_encodings.torch_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D, Summer\n",
        "\n",
        "class DiceyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiceyModel, self).__init__()\n",
        "        # Adjusted output channels to 20\n",
        "        self.rbp_conv1 = nn.Conv2d(19, 20, (5, 5), stride=1)\n",
        "        self.scc_conv1 = nn.Conv2d(19, 20, (5, 5), stride=1)\n",
        "\n",
        "        self.positional_encoding = PositionalEncoding1D(20)\n",
        "\n",
        "        # Corrected the shape of cls_token\n",
        "        self.rbp_cls_token = nn.Parameter(torch.zeros(1, 1, 20))\n",
        "        self.scc_cls_token = nn.Parameter(torch.zeros(1, 1, 20))\n",
        "\n",
        "        # Set batch_first=True\n",
        "        self.rbp_mha = nn.MultiheadAttention(20, 4, batch_first=True)\n",
        "        self.scc_mha = nn.MultiheadAttention(20, 4, batch_first=True)\n",
        "\n",
        "        self.rbp_mlp = nn.Sequential(\n",
        "            nn.Linear(20, 80), nn.GELU(), nn.Dropout(0.1), nn.Linear(80, 20)\n",
        "        )\n",
        "\n",
        "        self.scc_mlp = nn.Sequential(\n",
        "            nn.Linear(20, 80), nn.GELU(), nn.Dropout(0.1), nn.Linear(80, 20)\n",
        "        )\n",
        "\n",
        "        self.rbp_norm1 = nn.LayerNorm(20)\n",
        "        self.scc_norm1 = nn.LayerNorm(20)\n",
        "\n",
        "        self.rbp_norm2 = nn.LayerNorm(20)\n",
        "        self.scc_norm2 = nn.LayerNorm(20)\n",
        "\n",
        "        # Adjusted MLP input size to match concatenated embeddings for binary output\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(40, 24), nn.GELU(), nn.Dropout(0.3), nn.Linear(24, 1),\n",
        "            nn.Sigmoid()  # Added sigmoid activation for binary classification\n",
        "        )\n",
        "\n",
        "    def forward(self, rbp, scc):\n",
        "        rbp = self.rbp_conv1(rbp).squeeze(-1)\n",
        "        scc = self.scc_conv1(scc).squeeze(-1)\n",
        "\n",
        "        # GELU activation\n",
        "        rbp = nn.functional.gelu(rbp)\n",
        "        scc = nn.functional.gelu(scc)\n",
        "\n",
        "        # Reshape from (batch_size, 20, 26) to (batch_size, 26, 20)\n",
        "        rbp = rbp.permute(0, 2, 1)\n",
        "        scc = scc.permute(0, 2, 1)\n",
        "\n",
        "        # Apply positional encoding\n",
        "        rbp = rbp + self.positional_encoding(rbp)\n",
        "        scc = scc + self.positional_encoding(scc)\n",
        "\n",
        "        # Concatenate CLS token along the sequence dimension (dim=1)\n",
        "        rbp_cls_token = self.rbp_cls_token.expand(rbp.size(0), -1, -1)\n",
        "        scc_cls_token = self.scc_cls_token.expand(scc.size(0), -1, -1)\n",
        "        rbp = torch.cat((rbp_cls_token, rbp), dim=1)\n",
        "        scc = torch.cat((scc_cls_token, scc), dim=1)\n",
        "\n",
        "        # Apply multihead attention and LayerNorm\n",
        "        rbp_attn_output = self.rbp_mha(rbp, rbp, rbp)[0]\n",
        "        scc_attn_output = self.scc_mha(scc, scc, scc)[0]\n",
        "        rbp = self.rbp_norm1(rbp + rbp_attn_output)\n",
        "        scc = self.scc_norm1(scc + scc_attn_output)\n",
        "\n",
        "        # Apply MLP and LayerNorm\n",
        "        rbp_mlp_output = self.rbp_mlp(rbp)\n",
        "        scc_mlp_output = self.scc_mlp(scc)\n",
        "        rbp = self.rbp_norm2(rbp + rbp_mlp_output)\n",
        "        scc = self.scc_norm2(scc + scc_mlp_output)\n",
        "\n",
        "        # Corrected indexing to extract CLS token embeddings\n",
        "        embed = torch.cat((rbp[:, 0, :], scc[:, 0, :]), dim=1)\n",
        "        return self.mlp(embed)  # Output now is binary classification\n"
      ],
      "metadata": {
        "id": "Nwkq3qajG24L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming torch_scc, torch_rbp, and new_diags are your dataset tensors\n",
        "batch_size = 32\n",
        "test_split_ratio = 0.2  # 20% of the data for testing\n",
        "\n",
        "# Create TensorDataset\n",
        "dataset = TensorDataset(torch_scc, torch_rbp, new_diags)\n",
        "\n",
        "# Calculate the size of test and train sets\n",
        "test_size = int(test_split_ratio * len(dataset))\n",
        "train_size = len(dataset) - test_size\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create DataLoaders for both the training and testing sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = DiceyModel().to(\"cuda\")\n",
        "\n",
        "# Apply weight decay for L2 regularization (typically a small value)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "# Add learning rate scheduler\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Reduce LR by a factor of 0.1 every 10 epochs\n",
        "\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy for binary classification\n",
        "\n",
        "epochs = 100  # Example number of epochs\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Training loop\n",
        "    for batch_idx, (batch_scc, batch_rbp, batch_diags) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_scc, batch_rbp).squeeze()\n",
        "        loss = criterion(outputs, batch_diags.float())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation on test set with accuracy\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch_scc, batch_rbp, batch_diags in test_loader:\n",
        "            outputs = model(batch_scc, batch_rbp).squeeze()\n",
        "\n",
        "            # Apply a threshold of 0.5 to predict class labels\n",
        "            preds = (outputs > 0.5).float()\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(batch_diags.cpu().numpy())\n",
        "\n",
        "    # Calculate accuracy using sklearn's accuracy_score\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Step the scheduler (for StepLR)\n",
        "    scheduler.step()\n",
        "\n",
        "    # Alternatively, if using ReduceLROnPlateau scheduler, pass in the validation loss\n",
        "    # scheduler.step(avg_test_loss)\n"
      ],
      "metadata": {
        "id": "bGhAWyFYVAyC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}